{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Test-Deformation.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1nWptFlHxplTPQwi-irqwLgHaFKqNuz4Z","authorship_tag":"ABX9TyP6t3GQLpyLFImVdpnK5bka"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"UWQrt06UVSto","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636704877611,"user_tz":-540,"elapsed":3268,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"3de1dc46-a58e-462d-eee6-d448b02ede8d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"NMXMkMMZ1OxK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636704893052,"user_tz":-540,"elapsed":15445,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"215e06f7-1fec-496d-df37-a1ca0a474629"},"source":["!pip install tqdm\n","!pip install segmentation_models_pytorch\n","!pip install albumentations"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n","Collecting segmentation_models_pytorch\n","  Downloading segmentation_models_pytorch-0.2.0-py3-none-any.whl (87 kB)\n","\u001b[K     |████████████████████████████████| 87 kB 5.0 MB/s \n","\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n","  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n","Collecting pretrainedmodels==0.7.4\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 6.7 MB/s \n","\u001b[?25hCollecting timm==0.4.12\n","  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n","\u001b[K     |████████████████████████████████| 376 kB 49.0 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.11.1+cu111)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (1.10.0+cu111)\n","Collecting munch\n","  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.62.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (3.10.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.19.5)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12421 sha256=ead4aff1207281dfd96490b325c5afc7bd33a4017e531abeedc2297c038826e5\n","  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=6e478039a446464764bf35bbe11ee3493b00e06c7b2fb337eacac16eaf79a448\n","  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n","Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.0 timm-0.4.12\n","Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n","Collecting imgaug<0.2.7,>=0.2.5\n","  Downloading imgaug-0.2.6.tar.gz (631 kB)\n","\u001b[K     |████████████████████████████████| 631 kB 11.3 MB/s \n","\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.1.2.30)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n","Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.15.0)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.2.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.6.3)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.1.2)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.2.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.3.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.11.0)\n","Building wheels for collected packages: imgaug\n","  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654020 sha256=b9f5b06a4b18a3245bcb879b4e73fc8efae6cc0371c25454b5f9a7c83b8b1908\n","  Stored in directory: /root/.cache/pip/wheels/89/72/98/3ebfdba1069a9a8eaaa7ae7265cfd67d63ef0197aaee2e5f9c\n","Successfully built imgaug\n","Installing collected packages: imgaug\n","  Attempting uninstall: imgaug\n","    Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","Successfully installed imgaug-0.2.6\n"]}]},{"cell_type":"code","metadata":{"id":"Ze4jBsbgpKyg","executionInfo":{"status":"ok","timestamp":1636704907441,"user_tz":-540,"elapsed":14391,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["import os\n","import glob\n","import torch\n","import cv2 as cv\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from scipy.misc import electrocardiogram\n","from scipy.signal import find_peaks\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from torch.utils.data import Dataset"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSRtacK0oMJi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636704907441,"user_tz":-540,"elapsed":5,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"64fe9a0d-31d7-45c0-9653-3a8ab0657488"},"source":["cd drive/Shareddrives/KOHI_의료영상1팀/"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shareddrives/KOHI_의료영상1팀\n"]}]},{"cell_type":"code","metadata":{"id":"kxoJlFXqu1xM","executionInfo":{"status":"ok","timestamp":1636704907442,"user_tz":-540,"elapsed":4,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["image_dir = 'Training_Data/image'\n","mask_dir = 'Training_Data/mask'"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YyPvVAG-0G4m"},"source":["## Load Image"]},{"cell_type":"markdown","metadata":{"id":"SSRuoUB-RnQp"},"source":["load image를 할때 cv.imread로 미리 읽어오면 메모리가 부족할 수도 있는 현상이 발생할 수도 있으므로,\n","\n","imread는 dataloader의 __getitem__에서 해결"]},{"cell_type":"code","metadata":{"id":"lzKZvo9KITwf","executionInfo":{"status":"ok","timestamp":1636704923025,"user_tz":-540,"elapsed":15587,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["input_list = os.listdir(image_dir)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6hkgjGWpalf","executionInfo":{"status":"ok","timestamp":1636704923026,"user_tz":-540,"elapsed":7,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"d5d6647b-88b0-4205-8bcc-99fce5562580"},"source":["dataset = []\n","\n","\n","for input in tqdm(input_list):\n","  image = os.path.join(image_dir, input)\n","  label = os.path.join(mask_dir, input)\n","\n","  dataset.append({'name': input[:-4], 'image_path':image, 'label_path':label})"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 2324/2324 [00:00<00:00, 225502.30it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"nb2jC_IF5Yb0"},"source":["### split train/test data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7z8fkLH35XvA","executionInfo":{"status":"ok","timestamp":1636704923026,"user_tz":-540,"elapsed":4,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"081010af-c087-4495-81ac-89d72489359b"},"source":["num_train = int(len(dataset)*0.8)\n","num_rest = len(dataset)-num_train\n","\n","if num_rest % 2 == 1:\n","  num_val = int(num_rest/2)+1\n","  num_test = int(num_rest/2)\n","else:\n","  num_val = int(num_rest/2)\n","  num_test = int(num_rest/2)\n","\n","data_train = dataset[:num_train]\n","data_val = dataset[num_train:num_train+num_val]\n","data_test = dataset[num_train+num_val:]\n","print(f'train dataset 개수: {len(data_train)}, valid dataset 개수: {len(data_val)}, test dataset 개수: {len(data_test)}')\n","\n","lr = 1e-5\n","batch_size=8\n","train_continue= False\n","augment=False\n","\n","optim_mode = 'adam'\n","\n","model_sort = 'efficient'\n","\n","# ckpt_dir=f'이승아/checkpoint/U-Net/Dice/1e-05(padding)'\n","# result_dir =f'이승아/result/U-Net/Dice/1e-05(padding)'\n","\n","ckpt_dir=f'이승아/checkpoint/EfficientNet/V2'\n","\n","image_size = 256\n"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["train dataset 개수: 1859, valid dataset 개수: 233, test dataset 개수: 232\n"]}]},{"cell_type":"markdown","metadata":{"id":"gFUf7rQjJIF5"},"source":["## Augmentation"]},{"cell_type":"code","metadata":{"id":"0D8ERhw48qcd","executionInfo":{"status":"ok","timestamp":1636704923623,"user_tz":-540,"elapsed":600,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["import albumentations as A\n","from torch.utils.data import DataLoader"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMYX2TfM9F_1","executionInfo":{"status":"ok","timestamp":1636704923624,"user_tz":-540,"elapsed":4,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def _normalization(input):\n","  input = (input - input.min()) / (input.max() - input.min())\n","  return input\n","\n","def _standardization(input):\n","  input = (input - mean) / std\n","  return input\n","\n","def _to_tensor(input, label, name, h, w):\n","  input = input.astype('float32')\n","  label = label.astype('float32')\n","\n","  input = input.reshape((1, label.shape[0], label.shape[1]))\n","\n","  label = label.reshape((1, label.shape[0], label.shape[1]))\n","  \n","  data = {'name':name, 'input': torch.from_numpy(input), 'label': torch.from_numpy(label),\n","          'height': h, 'width': w}\n","  return data\n","\n","def _random_augment(input, label):\n","  h, w  = input.shape\n","  \n","  transform = A.Compose([A.HorizontalFlip(p=0.5),\n","                      A.VerticalFlip(p=0.5),\n","                      A.RandomCrop(height=int(h*0.8), width=int(w*0.8), p=0.5)\n","                      ]\n","                      , additional_targets={'label': 'image'})\n","\n","  augmented = transform(image=input, label=label)\n","\n","  input = augmented['image']\n","  label = augmented['label']\n","\n","  return input, label"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ccAqrF70tcST"},"source":["## Resize by aspect ratio and Padding Image"]},{"cell_type":"code","metadata":{"id":"jwq3WIQStbEa","executionInfo":{"status":"ok","timestamp":1636704923624,"user_tz":-540,"elapsed":4,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def resize_image(input, label):\n","  # shape: (height, width, channel)\n","  # aspect ratio를 고려하여 resize\n","  if input.shape[1] > input.shape[0]:   # height를 기준으로\n","    r = image_size / input.shape[1]\n","    dim = ( image_size, int(input.shape[0] * r))\n","  else:                                 # width를 기준으로\n","    r = image_size / input.shape[0]\n","    dim = (int(input.shape[1] * r), image_size)\n","    \n","  resized_input = cv.resize(input, dim, interpolation = cv.INTER_AREA)\n","  resized_label = cv.resize(label, dim, interpolation = cv.INTER_NEAREST)\n","  \n","  return resized_input, resized_label\n","\n","def padding_image(input, label):\n","  input_size = input.shape\n","  target_size = (image_size, image_size)\n","\n","  if input_size[1] < image_size:\n","    padding_range = int(target_size[1]-input_size[1])\n","  elif input_size[0] < image_size:\n","    padding_range = int(target_size[0]-input_size[0])\n","  else:\n","    return input, label\n","\n","  if padding_range%2 == 0:\n","    padding_size = (int(padding_range/2), int(padding_range/2))\n","  else:\n","    padding_size = (int(padding_range/2), int(padding_range/2)+1)\n","  \n","  if input_size[1] < image_size:\n","    npad= ((0,0),padding_size)\n","  elif input_size[0] < image_size:\n","    npad= (padding_size, (0,0))\n","\n","  padding_input = np.pad(input, npad,'constant', constant_values=(0))\n","  padding_label = np.pad(label, npad,'constant', constant_values=(0))\n","\n","  return padding_input, padding_label"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UM4PYZNw47T7"},"source":["## DataLoader"]},{"cell_type":"code","metadata":{"id":"U05CJ7uh27bq","executionInfo":{"status":"ok","timestamp":1636704923624,"user_tz":-540,"elapsed":4,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["# DataLoader\n","\n","row_ratio = 0.15\n","col_ratio = 0.15\n","\n","def label_preprocessing(label):\n","  label = label/255.\n","\n","  return label\n","\n","class VertebraeDataset(Dataset):\n","\n","  def __init__(self, data, augment=False):\n","    super(VertebraeDataset, self).__init__()\n","\n","    self.data = data      \n","    self.augment = augment\n","    self.image_size = image_size\n","\n","  def __getitem__(self, index):\n","    # Read input, label\n","    name = self.data[index]['name']\n","    input = cv.imread(self.data[index]['image_path'])\n","    h, w, _ = input.shape\n","    # print(h)\n","    # print(w)\n","    try:\n","      input = cv.cvtColor(input, cv.COLOR_BGR2GRAY)    \n","    except:\n","      print(self.data[index]['image_path'])\n","      exit(-1)\n","\n","    label = cv.imread(self.data[index]['label_path'])\n","    try:\n","      label = cv.cvtColor(label, cv.COLOR_BGR2GRAY)  \n","    except:\n","      print(self.data[index]['label_path'])\n","      exit(-1)\n","    label = label/255.\n","\n","    # # 영상 개선\n","    clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n","    input = clahe.apply(input)\n","    input = input / 255.0\n","    \n","    if self.augment:\n","      input, label = _random_augment(input, label)\n","      \n","    # Resize and pad Image\n","    input, label = resize_image(input, label)\n","    input, label = padding_image(input, label)\n","\n","    data = _to_tensor(input, label, name, h, w)\n","\n","    return data\n","\n","  def __len__(self):\n","    return len(self.data)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sja5vP-y6Hs2","executionInfo":{"status":"ok","timestamp":1636704927084,"user_tz":-540,"elapsed":3464,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def get_data(data, shuffle=True):\n","    ### Train Dataset 가져오기\n","    dataset = VertebraeDataset(data, augment=augment)\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)  \n","\n","    return dataset, loader\n","\n","# pytorch Dataloader\n","train_dataset, train_loader = get_data(data_train)\n","val_dataset, val_loader = get_data(data_val)\n","test_dataset, test_loader = get_data(data_test, shuffle=False)\n","\n","_data = test_dataset.__getitem__(0)\n","\n","\n","# test_dataset, test_loader = get_data(data_train[:1])"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8QfXtubLHJT"},"source":["## Parameter 설정\n"]},{"cell_type":"code","metadata":{"id":"JqQe3tOH2Kqw","executionInfo":{"status":"ok","timestamp":1636704928644,"user_tz":-540,"elapsed":1563,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["\n","import torch.nn.functional as F\n","import segmentation_models_pytorch as smp\n","from torch import nn"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"00eUF-si1h2J","executionInfo":{"status":"ok","timestamp":1636704929021,"user_tz":-540,"elapsed":5,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class DiceLoss(nn.Module):\n","    \"\"\"\n","    Dice score loss function\n","    \"\"\"\n","    def __init__(self):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = 1.0\n","\n","    def forward(self, output, label):\n","        assert output.size() == label.size()\n","        output = output[:, 0].contiguous().view(-1)\n","        label = label[:, 0].contiguous().view(-1)\n","        intersection = (output * label).sum()\n","        dsc = (2. * intersection + self.smooth) / (output.sum() + label.sum() + self.smooth)\n","\n","        return 1. - dsc\n","\n","\n","class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceBCELoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","\n","        # comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = torch.sigmoid(inputs)\n","\n","        # flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice_loss = 1 - (2. * intersection + smooth) /(inputs.sum() + targets.sum() + smooth)\n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        Dice_BCE = BCE + dice_loss\n","\n","        return Dice_BCE\n","\n","class Binary_Loss(nn.Module):\n","    def __init__(self):\n","        super(Binary_Loss, self).__init__()\n","        self.criterion = nn.BCEWithLogitsLoss()\n","\n","\n","    def forward(self, model_output, targets):\n","        loss = self.criterion(model_output, targets)\n","\n","       \n","        return loss\n","\n","bcedice_loss = DiceBCELoss().cuda()\n","binary_loss = Binary_Loss().cuda()\n","dsc_loss = DiceLoss().cuda()"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9zjD6UidJJpi"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"t7Ml0exeJLqf","executionInfo":{"status":"ok","timestamp":1636704929021,"user_tz":-540,"elapsed":4,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["from segmentation_models_pytorch.unet.model import Unet"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1LCPAOMgY8ac","executionInfo":{"status":"ok","timestamp":1636704939679,"user_tz":-540,"elapsed":10662,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"bc74c388-6c56-4937-aefa-4c23c99707b7"},"source":["from collections import OrderedDict\n","\n","if model_sort == 'unet':\n","  class UNet(nn.Module):\n","\n","      def __init__(self, in_channels=1, out_channels=1, init_features=32):\n","          super(UNet, self).__init__()\n","\n","          features = init_features\n","          self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n","          self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","          self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n","          self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","          self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n","          self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","          self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n","          self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","          self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n","\n","          self.upconv4 = nn.ConvTranspose2d(\n","              features * 16, features * 8, kernel_size=2, stride=2\n","          )\n","          self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n","          self.upconv3 = nn.ConvTranspose2d(\n","              features * 8, features * 4, kernel_size=2, stride=2\n","          )\n","          self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n","          self.upconv2 = nn.ConvTranspose2d(\n","              features * 4, features * 2, kernel_size=2, stride=2\n","          )\n","          self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n","          self.upconv1 = nn.ConvTranspose2d(\n","              features * 2, features, kernel_size=2, stride=2\n","          )\n","          self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n","\n","          self.conv = nn.Conv2d(\n","              in_channels=features, out_channels=out_channels, kernel_size=1\n","          )\n","\n","      def forward(self, x):\n","          enc1 = self.encoder1(x)\n","          enc2 = self.encoder2(self.pool1(enc1))\n","          enc3 = self.encoder3(self.pool2(enc2))\n","          enc4 = self.encoder4(self.pool3(enc3))\n","\n","          bottleneck = self.bottleneck(self.pool4(enc4))\n","\n","          dec4 = self.upconv4(bottleneck)\n","          dec4 = torch.cat((dec4, enc4), dim=1)\n","          dec4 = self.decoder4(dec4)\n","          dec3 = self.upconv3(dec4)\n","          dec3 = torch.cat((dec3, enc3), dim=1)\n","          dec3 = self.decoder3(dec3)\n","          dec2 = self.upconv2(dec3)\n","          dec2 = torch.cat((dec2, enc2), dim=1)\n","          dec2 = self.decoder2(dec2)\n","          dec1 = self.upconv1(dec2)\n","          dec1 = torch.cat((dec1, enc1), dim=1)\n","          dec1 = self.decoder1(dec1)\n","          return torch.sigmoid(self.conv(dec1))\n","\n","      @staticmethod\n","      def _block(in_channels, features, name):\n","          return nn.Sequential(\n","              OrderedDict(\n","                  [\n","                      (\n","                          name + \"conv1\",\n","                          nn.Conv2d(\n","                              in_channels=in_channels,\n","                              out_channels=features,\n","                              kernel_size=3,\n","                              padding=1,\n","                              bias=False,\n","                          ),\n","                      ),\n","                      (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n","                      (name + \"relu1\", nn.ReLU(inplace=True)),\n","                      (\n","                          name + \"conv2\",\n","                          nn.Conv2d(\n","                              in_channels=features,\n","                              out_channels=features,\n","                              kernel_size=3,\n","                              padding=1,\n","                              bias=False,\n","                          ),\n","                      ),\n","                      (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n","                      (name + \"relu2\", nn.ReLU(inplace=True)),\n","                  ]\n","              )\n","          )\n","  model = UNet().to(device)\n","  print('unet')\n","else:\n","  model = Unet(\n","    encoder_name=\"efficientnet-b5\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n","    encoder_weights=None,     # use `imagenet` pre-trained weights for encoder initialization\n","    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n","    classes=1,).to(device)\n","  print('efficient')\n","\n","# Optimizer Adam 설정\n","if optim_mode == 'adam':\n","  optim = torch.optim.Adam(model.parameters(), lr=lr)\n","elif optim_mode == 'sgd':\n","  optim = torch.optim.SGD(model.parameters(), lr=lr)\n","elif optim_mode == 'adamW':\n","  optim = torch.optim.Adam(model.parameters(), lr=lr)\n","print(optim_mode)"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["efficient\n","adam\n"]}]},{"cell_type":"code","metadata":{"id":"O3A2ecqdPuUt","executionInfo":{"status":"ok","timestamp":1636704939679,"user_tz":-540,"elapsed":9,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)  # Tensor를 numpy로 변환\n","fn_denorm = lambda x, mean, std: (x * std) + mean  # DeNomarlization\n","fn_class = lambda x: 1.0 * (x > 0.4)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"_6Jy2ctTXEv_","executionInfo":{"status":"ok","timestamp":1636704939680,"user_tz":-540,"elapsed":10,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["result_dir =f'이승아/result/EfficientNet/V2-300'\n","\n","def load_model(ckpt_dir, model, optim):\n","    if not os.path.exists(ckpt_dir):\n","        epoch = 0\n","        return model\n","\n","    ckpt_lst = os.listdir(ckpt_dir)\n","    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n","    print(os.path.join(ckpt_dir, ckpt_lst[-1]))\n","\n","    dict_model = torch.load(os.path.join(ckpt_dir, ckpt_lst[-1]))\n","    model.load_state_dict(dict_model['model'], strict=False)\n","    optim.load_state_dict(dict_model['optim'])\n","    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n","    print(\"Get saved weights successfully.\")\n","\n","    return model, optim, epoch\n","\n","def save_model(ckpt_dir, model, optim, epoch):\n","    if not os.path.exists(ckpt_dir):\n","        os.makedirs(ckpt_dir)\n","\n","    torch.save({'model': model.state_dict(), 'optim': optim.state_dict()},\n","                \"./%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n","    print(f'>> save model_{epoch}.pth')"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GxAdDAPKYbO0"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"MPXck0Vmo_sJ","executionInfo":{"status":"ok","timestamp":1636704939680,"user_tz":-540,"elapsed":9,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["import copy\n","\n","def metric(gt,pred):\n","    preds = pred.detach().numpy()\n","    gts = gt.detach().numpy()\n","\n","    pred = preds.astype(int)  # float data does not support bit_and and bit_or\n","    gdth = gts.astype(int)  # float data does not support bit_and and bit_or\n","    fp_array = copy.deepcopy(pred)  # keep pred unchanged\n","    fn_array = copy.deepcopy(gdth)\n","    gdth_sum = np.sum(gdth)\n","    pred_sum = np.sum(pred)\n","    intersection = gdth & pred\n","    union = gdth | pred\n","    intersection_sum = np.count_nonzero(intersection)\n","    union_sum = np.count_nonzero(union)\n","\n","    tp_array = intersection\n","\n","    tmp = pred - gdth\n","    fp_array[tmp < 1] = 0\n","\n","    tmp2 = gdth - pred\n","    fn_array[tmp2 < 1] = 0\n","\n","    tn_array = np.ones(gdth.shape) - union\n","\n","    tp, fp, fn, tn = np.sum(tp_array), np.sum(fp_array), np.sum(fn_array), np.sum(tn_array)\n","\n","    smooth = 0.001\n","    precision = tp / (pred_sum + smooth)\n","    recall = tp / (gdth_sum + smooth)\n","\n","    false_positive_rate = fp / (fp + tn + smooth)\n","    false_negtive_rate = fn / (fn + tp + smooth)\n","\n","    jaccard = intersection_sum / (union_sum + smooth)\n","    dice = 2 * intersection_sum / (gdth_sum + pred_sum + smooth)\n","    \n","    epsilon = 1e-5\n","    f1_score = 2 * (precision*recall) / (precision + recall + epsilon)\n","\n","    return false_positive_rate,false_negtive_rate,f1_score\n","    "],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_i-iyqjXUa7","executionInfo":{"status":"ok","timestamp":1636704939680,"user_tz":-540,"elapsed":9,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def numeric_score(output, label):\n","    FP = np.float(np.sum((output == 1) & (label == 0)))\n","    FN = np.float(np.sum((output == 0) & (label == 1)))\n","    if FP != 0.0 or FN != 0.0:\n","        pass\n","    TP = np.float(np.sum((output == 1) & (label == 1)))\n","    TN = np.float(np.sum((output == 0) & (label == 0)))\n","\n","    return FP, FN, TP, TN\n","\n","def get_score(output, label):\n","    FP, FN, TP, TN = numeric_score(output, label)\n","    N = FP + FN + TP + TN\n","\n","    epsilon = 1e-5\n","\n","    # Recall : TP / TP+FN\n","    recall = np.divide(TP, TP + FN + epsilon)\n","    # Precision : TP / TP+FP\n","    precision = np.divide(TP, (TP+FP+epsilon))\n","\n","    accuracy = np.divide((TP + TN), N+epsilon)\n","\n","    # F1 socre = 2 * (A interect B) / |A| + |B| = 2TP / 2TP + FP + FN\n","    f1_score = 2 * (precision*recall) / (precision + recall + epsilon)\n","    dice_coeff = 2*TP / (2*TP+FP+FN+epsilon)\n","\n","    # J(A,B) = | A intersect B | / | A union B |\n","    jaccard_score = TP / (TP+FN+FP+ epsilon)\n","\n","    return recall * 100, precision * 100, accuracy * 100, f1_score*100, jaccard_score*100\n"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"VlChbI22iP1K"},"source":["pip install plantcv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vqtkMwDnUrUp","executionInfo":{"status":"ok","timestamp":1636704956486,"user_tz":-540,"elapsed":1169,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["from plantcv import plantcv as pcv\n","from skimage import img_as_float, morphology\n","from skimage.color import gray2rgb\n","from cv2.ximgproc import thinning"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"gRePto7ukzaY","executionInfo":{"status":"ok","timestamp":1636704956486,"user_tz":-540,"elapsed":4,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def _normalise_image(image, *, image_cmap=None):\n","    image = img_as_float(image)\n","    if image.ndim == 2:\n","        if image_cmap is None:\n","            image = gray2rgb(image)\n","        else:\n","            image = plt.get_cmap(image_cmap)(image)[..., :3]\n","    return image"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"RpHEAQYukecm","executionInfo":{"status":"ok","timestamp":1636704957750,"user_tz":-540,"elapsed":1,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def overlay_skeleton_2d(image, skeleton, *,\n","                        image_cmap=None, color=(1, 0, 0), alpha=1,\n","                        dilate=0, axes=None):\n","    image = _normalise_image(image, image_cmap=image_cmap)\n","    skeleton = skeleton.astype(bool)\n","    if dilate > 0:\n","        selem = morphology.disk(dilate)\n","        skeleton = morphology.binary_dilation(skeleton, selem)\n","    image[skeleton] = alpha * np.array(color) + (1 - alpha) * image[skeleton]\n","    return image"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"ahVL5HKHNrkP","executionInfo":{"status":"ok","timestamp":1636705067449,"user_tz":-540,"elapsed":368,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def remove_horizontal_line(output):\n","  line_kernel = np.zeros((7, 7),dtype=np.uint8)\n","  line_kernel[3,...]=1\n","  x=cv.morphologyEx(output, cv.MORPH_OPEN, line_kernel ,iterations=1)\n","  _output= output - x\n","\n","  return _output "],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"cCQ0t1s479rS","executionInfo":{"status":"ok","timestamp":1636705414977,"user_tz":-540,"elapsed":385,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["final_result_dir = './Evaluate result/PadChest/line'\n","\n","def post_processing(output):\n","  \n","  # # Opening Image\n","  kernel = np.ones((9, 9), np.uint8)\n","  result_opening = cv.morphologyEx(output, cv.MORPH_OPEN, kernel)\n","  \n","  # Thining Image\n","  result_thinning = thinning((result_opening*255).astype('uint8'))\n","  # Remove horizontal_line\n","  result_thinning = remove_horizontal_line(result_thinning)\n","\n","  # # Skeletonizing Image\n","  # result_skeletonizing = (skeletonize(result_opening)*255).astype('uint8')\n","\n","  return result_opening, result_thinning\n","\n","def save_predictedMask(input, label, output, name, origin_height, origin_width):\n","    for idx in range(label.shape[0]):\n","        file_name = name[idx]\n","        _input = input[idx].astype('uint8').squeeze()\n","\n","        result_opening, result_thinning = post_processing(output[idx].squeeze())\n","        # pruned_skeleton, segmented_img, segment_objects = pcv.morphology.prune(skel_img=result_skeletonizing, size=300)\n","\n","        overlay_image = overlay_skeleton_2d(_input, result_thinning, dilate=1)\n","        cv.imwrite(os.path.join('./Evaluate result/PadChest/mask', file_name+'.png'), result_opening*255)\n","        cv.imwrite(os.path.join('./Evaluate result/PadChest/line/256', file_name+'.png'), result_thinning)\n","        \n","        h = origin_height[idx]\n","        w = origin_width[idx]\n","        dim = (w, h)\n","        resized_result_thinning = cv.resize(result_thinning, dim, interpolation = cv.INTER_NEAREST)\n","        cv.imwrite(os.path.join('./Evaluate result/PadChest/line/origin_size', file_name+'.png'), resized_result_thinning)\n","        \n","        fig, ax = plt.subplots()\n","        plt.axis('off'), plt.xticks([]), plt.yticks([])\n","        plt.tight_layout()\n","        plt.imshow(overlay_image)\n","        plt.savefig(os.path.join('./Evaluate result/PadChest/overlay', file_name+'.png'),bbox_inches='tight')\n","        plt.close()"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"id":"zBQ_Qrw98HGS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636706299213,"user_tz":-540,"elapsed":881749,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"3bfb2ab0-672d-4d17-a2f8-5bbb1dcd4905"},"source":["st_epoch=0\n","\n","print(result_dir)\n","model, optim, st_epoch = load_model(ckpt_dir=ckpt_dir, model=model, optim=optim)\n","\n","with torch.no_grad():\n","    model.eval()\n","\n","    loss_arr = []\n","    total_recall = []\n","    total_precision = []\n","    total_f1_score = []\n","    total_jaccard = []\n","\n","    for iter, data in enumerate(tqdm(test_loader), 1):\n","        name = data['name']\n","        input = data['input'].to(device)\n","        label = data['label'].to(device)\n","\n","        origin_height = data['height']\n","        origin_width = data['width']\n","        \n","        output = model(input)\n","\n","        logit = torch.sigmoid(output)\n","        output = logit.clone()\n","        output[output>0.5] = 1\n","        output[output<=0.5] = 0\n","       \n","        false_positive_rate,false_negtive_rate,f1_score = metric(label.cpu(), output.cpu())\n","        \n","        label = fn_tonumpy(label)\n","        input = fn_tonumpy(input*255)\n","        output = fn_tonumpy(output)\n","        \n","        save_predictedMask(input, label, output, name, origin_height, origin_width)"],"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["이승아/result/EfficientNet/V2-300\n","이승아/checkpoint/EfficientNet/V2/model_epoch300.pth\n","Get saved weights successfully.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29/29 [14:40<00:00, 30.35s/it]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ki8Y2LrjooXV"},"source":["# Save final output\n"]},{"cell_type":"code","metadata":{"id":"Ftz-iFJ5qPUt"},"source":["result_dir = './Evaluate result/PadChest/mask'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-fqnCL1pFDr"},"source":["def save_final_output(name, output, origin_height, origin_width):\n","    output *= 255\n","    for idx in range(label.shape[0]):\n","        file_name = name[idx]\n","\n","        h = origin_height[idx]\n","        w = origin_width[idx]\n","        dim = (w, h)\n","\n","        _output = output[idx].squeeze()\n","        # _output = cv.resize(_output, dim, interpolation = cv.INTER_NEAREST)\n","        \n","        cv.imwrite(os.path.join(result_dir, file_name+'.png'), _output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_XEkTakpDhH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636546147334,"user_tz":-540,"elapsed":125874,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"bb9fb64e-6931-41f0-a956-5b0b7aeec9cb"},"source":["st_epoch=0\n","\n","model, optim, st_epoch = load_model(ckpt_dir=ckpt_dir, model=model, optim=optim)\n","\n","with torch.no_grad():\n","    model.eval()\n","\n","    loss_arr = []\n","    total_recall = []\n","    total_precision = []\n","    total_f1_score = []\n","    total_jaccard = []\n","\n","    for iter, data in enumerate(tqdm(test_loader), 1):\n","        name = data['name']\n","\n","        origin_height = data['height']\n","        origin_width = data['width']\n","\n","        input = data['input'].to(device)\n","        label = data['label'].to(device)\n","\n","        output = model(input)\n","\n","        loss = bcedice_loss(output, label)\n","        loss_arr += [loss.item()]\n","\n","        # for metrics\n","        logit = torch.sigmoid(output)\n","        output = logit.clone()\n","        output[output>0.5] = 1\n","        output[output<=0.5] = 0\n","\n","        # draw_ROC_curve(output, label)\n","        # recall, precision, accuracy, f1_score, jaccard = get_score(output, label)\n","        false_positive_rate,false_negtive_rate,f1_score = metric(label.cpu(), output.cpu())\n","        \n","        label = fn_tonumpy(label)\n","        input = fn_tonumpy(input*255)\n","        output = fn_tonumpy(output)\n","\n","        save_final_output(name, output, origin_height, origin_width)\n","        \n","        # save_predictedMask(input, label, output, iter)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["이승아/checkpoint/EfficientNet/V2/model_epoch300.pth\n","Get saved weights successfully.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 29/29 [02:04<00:00,  4.30s/it]\n"]}]},{"cell_type":"code","metadata":{"id":"8tWgv4Q7_jEs"},"source":["len(os.listdir(result_dir))"],"execution_count":null,"outputs":[]}]}