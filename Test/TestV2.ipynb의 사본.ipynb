{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TestV2.ipynb의 사본","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NMXMkMMZ1OxK","executionInfo":{"status":"ok","timestamp":1636159497678,"user_tz":-540,"elapsed":12642,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"0714cc77-8832-46a0-b934-50c6b52132ea"},"source":["!pip install tqdm\n","!pip install segmentation_models_pytorch\n","!pip install albumentations"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n","Collecting segmentation_models_pytorch\n","  Downloading segmentation_models_pytorch-0.2.0-py3-none-any.whl (87 kB)\n","\u001b[K     |████████████████████████████████| 87 kB 5.0 MB/s \n","\u001b[?25hCollecting pretrainedmodels==0.7.4\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 7.7 MB/s \n","\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.10.0+cu111)\n","Collecting timm==0.4.12\n","  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n","\u001b[K     |████████████████████████████████| 376 kB 58.3 MB/s \n","\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n","  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (1.9.0+cu111)\n","Collecting munch\n","  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.62.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.19.5)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12421 sha256=622545897450f5fb8d85096b21f72664811cfdb7c2b5cc937c5ca7767baa9271\n","  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=672cddd2dad0854aa4d09be21aa90fa29dd71aec2527838cca12ec51717af3a0\n","  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n","Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.0 timm-0.4.12\n","Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.1.2.30)\n","Collecting imgaug<0.2.7,>=0.2.5\n","  Downloading imgaug-0.2.6.tar.gz (631 kB)\n","\u001b[K     |████████████████████████████████| 631 kB 7.5 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.15.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.6.3)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.1.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.1)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.2.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.3.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\n","Building wheels for collected packages: imgaug\n","  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654020 sha256=50237db6fea98b03956e76786e1e3c2c03ca1ac01edfd959cd81b4e701f433ea\n","  Stored in directory: /root/.cache/pip/wheels/89/72/98/3ebfdba1069a9a8eaaa7ae7265cfd67d63ef0197aaee2e5f9c\n","Successfully built imgaug\n","Installing collected packages: imgaug\n","  Attempting uninstall: imgaug\n","    Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","Successfully installed imgaug-0.2.6\n"]}]},{"cell_type":"code","metadata":{"id":"Ze4jBsbgpKyg"},"source":["import os\n","import glob\n","import torch\n","import cv2 as cv\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from scipy.misc import electrocardiogram\n","from scipy.signal import find_peaks\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from torch.utils.data import Dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BSRtacK0oMJi","executionInfo":{"status":"ok","timestamp":1636159510613,"user_tz":-540,"elapsed":9,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"68509e32-01b3-468a-cbb9-2c617b708449"},"source":["cd drive/Shareddrives/KOHI_의료영상1팀/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shareddrives/KOHI_의료영상1팀\n"]}]},{"cell_type":"code","metadata":{"id":"kxoJlFXqu1xM"},"source":["image_dir = 'Training_Data/image'\n","mask_dir = 'Training_Data/mask'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YyPvVAG-0G4m"},"source":["## Load Image"]},{"cell_type":"markdown","metadata":{"id":"SSRuoUB-RnQp"},"source":["load image를 할때 cv.imread로 미리 읽어오면 메모리가 부족할 수도 있는 현상이 발생할 수도 있으므로,\n","\n","imread는 dataloader의 __getitem__에서 해결"]},{"cell_type":"code","metadata":{"id":"lzKZvo9KITwf"},"source":["input_list = os.listdir(image_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ckwUKWBpXTQ"},"source":["input_list.sort()\n","input_list = input_list[:917]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6hkgjGWpalf","executionInfo":{"status":"ok","timestamp":1636159526067,"user_tz":-540,"elapsed":10,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"cc48911c-2e03-48c4-85c9-c6486b397df5"},"source":["dataset = []\n","\n","\n","for input in tqdm(input_list):\n","  image = os.path.join(image_dir, input)\n","  label = os.path.join(mask_dir, input)\n","\n","  dataset.append({'name': input[:-4], 'image_path':image, 'label_path':label})"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 917/917 [00:00<00:00, 287715.20it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"nb2jC_IF5Yb0"},"source":["### split train/test data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7z8fkLH35XvA","executionInfo":{"status":"ok","timestamp":1636159859684,"user_tz":-540,"elapsed":389,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"adc308b0-9979-49ed-c5fa-403cbe573db7"},"source":["num_train = int(len(dataset)*0.8)\n","num_rest = len(dataset)-num_train\n","\n","if num_rest % 2 == 1:\n","  num_val = int(num_rest/2)+1\n","  num_test = int(num_rest/2)\n","else:\n","  num_val = int(num_rest/2)\n","  num_test = int(num_rest/2)\n","\n","data_train = dataset[:num_train]\n","data_val = dataset[num_train:num_train+num_val]\n","data_test = dataset[num_train+num_val:]\n","print(f'train dataset 개수: {len(data_train)}, valid dataset 개수: {len(data_val)}, test dataset 개수: {len(data_test)}')\n","\n","lr = 1e-5\n","batch_size=8\n","train_continue= False\n","augment=False\n","\n","optim_mode = 'adam'\n","\n","model_sort = 'unet'\n","\n","ckpt_dir=f'이승아/checkpoint/U-Net/Dice/1e-4(origin)'\n","result_dir =f'이승아/result/U-Net/Dice/1e-4(origin)'\n","\n","# ckpt_dir=f'이승아/checkpoint/EfficientNet/BCEDice/1e-5(origin)'\n","# result_dir =f'이승아/result/EfficientNet/BCEDice/1e-5(origin)'\n","\n","image_size = 256\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train dataset 개수: 733, valid dataset 개수: 92, test dataset 개수: 92\n"]}]},{"cell_type":"markdown","metadata":{"id":"gFUf7rQjJIF5"},"source":["## Augmentation"]},{"cell_type":"code","metadata":{"id":"0D8ERhw48qcd"},"source":["import albumentations as A\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMYX2TfM9F_1"},"source":["def _normalization(input):\n","  input = (input - input.min()) / (input.max() - input.min())\n","  return input\n","\n","def _standardization(input):\n","  input = (input - mean) / std\n","  return input\n","\n","def _to_tensor(input, label, name):\n","  input = input.astype('float32')\n","  label = label.astype('float32')\n","\n","  input = input.reshape((1, label.shape[0], label.shape[1]))\n","\n","  label = label.reshape((1, label.shape[0], label.shape[1]))\n","  \n","  data = {'name':name, 'input': torch.from_numpy(input), 'label': torch.from_numpy(label)}\n","  return data\n","\n","def _random_augment(input, label):\n","  h, w  = input.shape\n","  \n","  transform = A.Compose([A.HorizontalFlip(p=0.5),\n","                      A.VerticalFlip(p=0.5),\n","                      A.RandomCrop(height=int(h*0.8), width=int(w*0.8), p=0.5)\n","                      ]\n","                      , additional_targets={'label': 'image'})\n","\n","  augmented = transform(image=input, label=label)\n","\n","  input = augmented['image']\n","  label = augmented['label']\n","\n","  return input, label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ccAqrF70tcST"},"source":["## Resize by aspect ratio and Padding Image"]},{"cell_type":"code","metadata":{"id":"jwq3WIQStbEa"},"source":["def resize_image(input, label):\n","  # shape: (height, width, channel)\n","  # aspect ratio를 고려하여 resize\n","  if input.shape[1] > input.shape[0]:   # height를 기준으로\n","    r = image_size / input.shape[1]\n","    dim = ( image_size, int(input.shape[0] * r))\n","  else:                                 # width를 기준으로\n","    r = image_size / input.shape[0]\n","    dim = (int(input.shape[1] * r), image_size)\n","    \n","  resized_input = cv.resize(input, dim, interpolation = cv.INTER_AREA)\n","  resized_label = cv.resize(label, dim, interpolation = cv.INTER_NEAREST)\n","  \n","  return resized_input, resized_label\n","\n","def padding_image(input, label):\n","  input_size = input.shape\n","  target_size = (image_size, image_size)\n","\n","  if input_size[1] < image_size:\n","    padding_range = int(target_size[1]-input_size[1])\n","  elif input_size[0] < image_size:\n","    padding_range = int(target_size[0]-input_size[0])\n","  else:\n","    return input, label\n","\n","  if padding_range%2 == 0:\n","    padding_size = (int(padding_range/2), int(padding_range/2))\n","  else:\n","    padding_size = (int(padding_range/2), int(padding_range/2)+1)\n","  \n","  if input_size[1] < image_size:\n","    npad= ((0,0),padding_size)\n","  elif input_size[0] < image_size:\n","    npad= (padding_size, (0,0))\n","\n","  padding_input = np.pad(input, npad,'constant', constant_values=(0))\n","  padding_label = np.pad(label, npad,'constant', constant_values=(0))\n","\n","  return padding_input, padding_label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UM4PYZNw47T7"},"source":["## DataLoader"]},{"cell_type":"code","metadata":{"id":"j3Ve60jTN5C5"},"source":["def resize_image(input, label):\n","  # shape: (height, width, channel)\n","  # aspect ratio를 고려하여 resize\n","  if input.shape[1] > input.shape[0]:   # height를 기준으로\n","    r = image_size / input.shape[1]\n","    dim = ( image_size, int(input.shape[0] * r))\n","  else:                                 # width를 기준으로\n","    r = image_size / input.shape[0]\n","    dim = (int(input.shape[1] * r), image_size)\n","    \n","  resized_input = cv.resize(input, dim, interpolation = cv.INTER_AREA)\n","  resized_label = cv.resize(label, dim, interpolation = cv.INTER_NEAREST)\n","  \n","  return resized_input, resized_label\n","\n","def padding_image(input, label):\n","  input_size = input.shape\n","  target_size = (image_size, image_size)\n","\n","  if input_size[1] < image_size:\n","    padding_range = int(target_size[1]-input_size[1])\n","  elif input_size[0] < image_size:\n","    padding_range = int(target_size[0]-input_size[0])\n","  else:\n","    return input, label\n","\n","  if padding_range%2 == 0:\n","    padding_size = (int(padding_range/2), int(padding_range/2))\n","  else:\n","    padding_size = (int(padding_range/2), int(padding_range/2)+1)\n","  \n","  if input_size[1] < image_size:\n","    npad= ((0,0),padding_size)\n","  elif input_size[0] < image_size:\n","    npad= (padding_size, (0,0))\n","\n","  padding_input = np.pad(input, npad,'constant', constant_values=(0))\n","  padding_label = np.pad(label, npad,'constant', constant_values=(0))\n","\n","  return padding_input, padding_label"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U05CJ7uh27bq"},"source":["# DataLoader\n","\n","row_ratio = 0.15\n","col_ratio = 0.15\n","\n","def label_preprocessing(label):\n","  label = label/255.\n","\n","  return label\n","\n","class VertebraeDataset(Dataset):\n","\n","  def __init__(self, data, augment=False):\n","    super(VertebraeDataset, self).__init__()\n","\n","    self.data = data      \n","    self.augment = augment\n","    self.image_size = image_size\n","\n","  def __getitem__(self, index):\n","    # Read input, label\n","    name = self.data[index]['name']\n","    input = cv.imread(self.data[index]['image_path'])\n","    try:\n","      input = cv.cvtColor(input, cv.COLOR_BGR2GRAY)    \n","    except:\n","      print(self.data[index]['image_path'])\n","      exit(-1)\n","    input = input / 255.0\n","\n","    label = cv.imread(self.data[index]['label_path'])\n","    try:\n","      label = cv.cvtColor(label, cv.COLOR_BGR2GRAY)  \n","    except:\n","      print(self.data[index]['label_path'])\n","      exit(-1)\n","    label = label/255.\n","\n","    # # 영상 개선\n","    # clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n","    # input = clahe.apply(input)\n","    \n","    if self.augment:\n","      input, label = _random_augment(input, label)\n","      \n","    # Resize and pad Image\n","    input, label = resize_image(input, label)\n","    input, label = padding_image(input, label)\n","\n","    data = _to_tensor(input, label, name)\n","\n","    return data\n","\n","  def __len__(self):\n","    return len(self.data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sja5vP-y6Hs2"},"source":["def get_data(data, shuffle=True):\n","    ### Train Dataset 가져오기\n","    dataset = VertebraeDataset(data, augment=augment)\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)  \n","\n","    return dataset, loader\n","\n","# pytorch Dataloader\n","train_dataset, train_loader = get_data(data_train)\n","val_dataset, val_loader = get_data(data_val)\n","test_dataset, test_loader = get_data(data_test, shuffle=False)\n","\n","_data = test_dataset.__getitem__(0)\n","\n","\n","# test_dataset, test_loader = get_data(data_train[:1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8QfXtubLHJT"},"source":["## Parameter 설정\n"]},{"cell_type":"code","metadata":{"id":"JqQe3tOH2Kqw"},"source":["\n","import torch.nn.functional as F\n","import segmentation_models_pytorch as smp\n","from torch import nn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"00eUF-si1h2J"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class DiceLoss(nn.Module):\n","    \"\"\"\n","    Dice score loss function\n","    \"\"\"\n","    def __init__(self):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = 1.0\n","\n","    def forward(self, output, label):\n","        assert output.size() == label.size()\n","        output = output[:, 0].contiguous().view(-1)\n","        label = label[:, 0].contiguous().view(-1)\n","        intersection = (output * label).sum()\n","        dsc = (2. * intersection + self.smooth) / (output.sum() + label.sum() + self.smooth)\n","\n","        return 1. - dsc\n","\n","\n","class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceBCELoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","\n","        # comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = torch.sigmoid(inputs)\n","\n","        # flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice_loss = 1 - (2. * intersection + smooth) /(inputs.sum() + targets.sum() + smooth)\n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        Dice_BCE = BCE + dice_loss\n","\n","        return Dice_BCE\n","\n","class Binary_Loss(nn.Module):\n","    def __init__(self):\n","        super(Binary_Loss, self).__init__()\n","        self.criterion = nn.BCEWithLogitsLoss()\n","\n","\n","    def forward(self, model_output, targets):\n","        loss = self.criterion(model_output, targets)\n","\n","       \n","        return loss\n","\n","bcedice_loss = DiceBCELoss().cuda()\n","binary_loss = Binary_Loss().cuda()\n","dsc_loss = DiceLoss().cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9zjD6UidJJpi"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"t7Ml0exeJLqf"},"source":["from segmentation_models_pytorch.unet.model import Unet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1LCPAOMgY8ac","executionInfo":{"status":"ok","timestamp":1636159861061,"user_tz":-540,"elapsed":531,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"0faa3b85-eb5f-498c-ee42-be86017e8b46"},"source":["from collections import OrderedDict\n","\n","if model_sort == 'unet':\n","  class UNet(nn.Module):\n","\n","      def __init__(self, in_channels=1, out_channels=1, init_features=32):\n","          super(UNet, self).__init__()\n","\n","          features = init_features\n","          self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n","          self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","          self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n","          self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","          self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n","          self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","          self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n","          self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","          self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n","\n","          self.upconv4 = nn.ConvTranspose2d(\n","              features * 16, features * 8, kernel_size=2, stride=2\n","          )\n","          self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n","          self.upconv3 = nn.ConvTranspose2d(\n","              features * 8, features * 4, kernel_size=2, stride=2\n","          )\n","          self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n","          self.upconv2 = nn.ConvTranspose2d(\n","              features * 4, features * 2, kernel_size=2, stride=2\n","          )\n","          self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n","          self.upconv1 = nn.ConvTranspose2d(\n","              features * 2, features, kernel_size=2, stride=2\n","          )\n","          self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n","\n","          self.conv = nn.Conv2d(\n","              in_channels=features, out_channels=out_channels, kernel_size=1\n","          )\n","\n","      def forward(self, x):\n","          enc1 = self.encoder1(x)\n","          enc2 = self.encoder2(self.pool1(enc1))\n","          enc3 = self.encoder3(self.pool2(enc2))\n","          enc4 = self.encoder4(self.pool3(enc3))\n","\n","          bottleneck = self.bottleneck(self.pool4(enc4))\n","\n","          dec4 = self.upconv4(bottleneck)\n","          dec4 = torch.cat((dec4, enc4), dim=1)\n","          dec4 = self.decoder4(dec4)\n","          dec3 = self.upconv3(dec4)\n","          dec3 = torch.cat((dec3, enc3), dim=1)\n","          dec3 = self.decoder3(dec3)\n","          dec2 = self.upconv2(dec3)\n","          dec2 = torch.cat((dec2, enc2), dim=1)\n","          dec2 = self.decoder2(dec2)\n","          dec1 = self.upconv1(dec2)\n","          dec1 = torch.cat((dec1, enc1), dim=1)\n","          dec1 = self.decoder1(dec1)\n","          return torch.sigmoid(self.conv(dec1))\n","\n","      @staticmethod\n","      def _block(in_channels, features, name):\n","          return nn.Sequential(\n","              OrderedDict(\n","                  [\n","                      (\n","                          name + \"conv1\",\n","                          nn.Conv2d(\n","                              in_channels=in_channels,\n","                              out_channels=features,\n","                              kernel_size=3,\n","                              padding=1,\n","                              bias=False,\n","                          ),\n","                      ),\n","                      (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n","                      (name + \"relu1\", nn.ReLU(inplace=True)),\n","                      (\n","                          name + \"conv2\",\n","                          nn.Conv2d(\n","                              in_channels=features,\n","                              out_channels=features,\n","                              kernel_size=3,\n","                              padding=1,\n","                              bias=False,\n","                          ),\n","                      ),\n","                      (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n","                      (name + \"relu2\", nn.ReLU(inplace=True)),\n","                  ]\n","              )\n","          )\n","  model = UNet().to(device)\n","else:\n","  model = Unet(\n","    encoder_name=\"efficientnet-b5\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n","    encoder_weights=None,     # use `imagenet` pre-trained weights for encoder initialization\n","    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n","    classes=1,).to(device)\n","\n","# Optimizer Adam 설정\n","if optim_mode == 'adam':\n","  optim = torch.optim.Adam(model.parameters(), lr=lr)\n","elif optim_mode == 'sgd':\n","  optim = torch.optim.SGD(model.parameters(), lr=lr)\n","elif optim_mode == 'adamW':\n","  optim = torch.optim.Adam(model.parameters(), lr=lr)\n","print(optim_mode)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["adam\n"]}]},{"cell_type":"code","metadata":{"id":"O3A2ecqdPuUt"},"source":["fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)  # Tensor를 numpy로 변환\n","fn_denorm = lambda x, mean, std: (x * std) + mean  # DeNomarlization\n","fn_class = lambda x: 1.0 * (x > 0.4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_6Jy2ctTXEv_"},"source":["def load_model(ckpt_dir, model, optim):\n","    if not os.path.exists(ckpt_dir):\n","        epoch = 0\n","        return model\n","\n","    ckpt_lst = os.listdir(ckpt_dir)\n","    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n","    print(os.path.join(ckpt_dir, ckpt_lst[-1]))\n","\n","    dict_model = torch.load(os.path.join(ckpt_dir, ckpt_lst[-1]))\n","    model.load_state_dict(dict_model['model'], strict=False)\n","    optim.load_state_dict(dict_model['optim'])\n","    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n","    print(\"Get saved weights successfully.\")\n","\n","    return model, optim, epoch\n","\n","def save_model(ckpt_dir, model, optim, epoch):\n","    if not os.path.exists(ckpt_dir):\n","        os.makedirs(ckpt_dir)\n","\n","    torch.save({'model': model.state_dict(), 'optim': optim.state_dict()},\n","                \"./%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n","    print(f'>> save model_{epoch}.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GxAdDAPKYbO0"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"h_i-iyqjXUa7"},"source":["def numeric_score(output, label):\n","    FP = np.float(np.sum((output == 1) & (label == 0)))\n","    FN = np.float(np.sum((output == 0) & (label == 1)))\n","    if FP != 0.0 or FN != 0.0:\n","        pass\n","    TP = np.float(np.sum((output == 1) & (label == 1)))\n","    TN = np.float(np.sum((output == 0) & (label == 0)))\n","\n","    return FP, FN, TP, TN\n","\n","def get_score(output, label):\n","    FP, FN, TP, TN = numeric_score(output, label)\n","    N = FP + FN + TP + TN\n","\n","    epsilon = 1e-5\n","\n","    # Recall : TP / TP+FN\n","    recall = np.divide(TP, TP + FN + epsilon)\n","    # Precision : TP / TP+FP\n","    precision = np.divide(TP, (TP+FP+epsilon))\n","\n","    accuracy = np.divide((TP + TN), N+epsilon)\n","\n","    # F1 socre = 2 * (A interect B) / |A| + |B| = 2TP / 2TP + FP + FN\n","    f1_score = 2 * (precision*recall) / (precision + recall + epsilon)\n","    dice_coeff = 2*TP / (2*TP+FP+FN+epsilon)\n","\n","    # J(A,B) = | A intersect B | / | A union B |\n","    jaccard_score = TP / (TP+FN+FP+ epsilon)\n","\n","    return recall * 100, precision * 100, accuracy * 100, f1_score*100, jaccard_score*100\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cCQ0t1s479rS"},"source":["def save_predictedMask(input, label, output, name):\n","    for idx in range(label.shape[0]):\n","        file_name = name[idx]\n","        plt.figure(figsize=(21, 7))  \n","        plt.subplot(131)\n","        plt.imshow(input[idx].astype('uint8').squeeze(), cmap='gray')\n","        plt.title('input')\n","\n","        plt.subplot(132)\n","        plt.imshow(label[idx].squeeze(), cmap='gray')\n","        plt.title('label')\n","\n","        plt.subplot(133)    \n","        plt.imshow(output[idx].squeeze(), cmap='gray')\n","        plt.title('output')\n","        \n","        # plt.show()\n","        plt.axis('off'), plt.xticks([]), plt.yticks([])\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(result_dir, f'{file_name}.png'), pad_inches=0.1)\n","        plt.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"IkFoH0nJdnc9","executionInfo":{"status":"ok","timestamp":1636159861062,"user_tz":-540,"elapsed":6,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"6e6c74bb-a2e9-4048-8ae7-55680d66160e"},"source":["result_dir"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'이승아/result/U-Net/Dice/1e-4(origin)'"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"MPXck0Vmo_sJ"},"source":["import copy\n","\n","def metric(gt,pred):\n","    preds = pred.detach().numpy()\n","    gts = gt.detach().numpy()\n","\n","    pred = preds.astype(int)  # float data does not support bit_and and bit_or\n","    gdth = gts.astype(int)  # float data does not support bit_and and bit_or\n","    fp_array = copy.deepcopy(pred)  # keep pred unchanged\n","    fn_array = copy.deepcopy(gdth)\n","    gdth_sum = np.sum(gdth)\n","    pred_sum = np.sum(pred)\n","    intersection = gdth & pred\n","    union = gdth | pred\n","    intersection_sum = np.count_nonzero(intersection)\n","    union_sum = np.count_nonzero(union)\n","\n","    tp_array = intersection\n","\n","    tmp = pred - gdth\n","    fp_array[tmp < 1] = 0\n","\n","    tmp2 = gdth - pred\n","    fn_array[tmp2 < 1] = 0\n","\n","    tn_array = np.ones(gdth.shape) - union\n","\n","    tp, fp, fn, tn = np.sum(tp_array), np.sum(fp_array), np.sum(fn_array), np.sum(tn_array)\n","\n","    smooth = 0.001\n","    precision = tp / (pred_sum + smooth)\n","    recall = tp / (gdth_sum + smooth)\n","\n","    false_positive_rate = fp / (fp + tn + smooth)\n","    false_negtive_rate = fn / (fn + tp + smooth)\n","\n","    jaccard = intersection_sum / (union_sum + smooth)\n","    dice = 2 * intersection_sum / (gdth_sum + pred_sum + smooth)\n","    \n","    epsilon = 1e-5\n","    f1_score = 2 * (precision*recall) / (precision + recall + epsilon)\n","\n","    return false_positive_rate,false_negtive_rate,f1_score\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zBQ_Qrw98HGS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636159918273,"user_tz":-540,"elapsed":57216,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"5ec05ea0-680f-407d-c997-0ec501b63c28"},"source":["st_epoch=0\n","\n","model, optim, st_epoch = load_model(ckpt_dir=ckpt_dir, model=model, optim=optim)\n","\n","with torch.no_grad():\n","    model.eval()\n","\n","    loss_arr = []\n","    total_recall = []\n","    total_precision = []\n","    total_f1_score = []\n","    total_jaccard = []\n","\n","    for iter, data in enumerate(tqdm(test_loader), 1):\n","        name = data['name']\n","        input = data['input'].to(device)\n","        label = data['label'].to(device)\n","        \n","        output = model(input)\n","\n","        if model_sort == 'unet':\n","          loss = dsc_loss(output, label)\n","          \n","          # for metrics\n","          output[output>0.5] = 1\n","          output[output<=0.5] = 0\n","          \n","        else:\n","          loss = bcedice_loss(output, label)\n","          loss_arr += [loss.item()]\n","      \n","          # for metrics\n","          logit = torch.sigmoid(output)\n","          output = logit.clone()\n","          output[output>0.5] = 1\n","          output[output<=0.5] = 0\n","\n","        false_positive_rate,false_negtive_rate,f1_score = metric(label.cpu(), output.cpu())\n","        \n","        label = fn_tonumpy(label)\n","        input = fn_tonumpy(input*255)\n","        output = fn_tonumpy(output)\n","        \n","        save_predictedMask(input, label, output, name)\n","\n","#         print(\"AVERAGE TEST: BATCH %04d / %04d | LOSS %.4f\" % (iter, num_batch_test, np.mean(loss_arr)))\n","\n","# print(\"\\nTEST: BATCH %04d / %04d | LOSS %.4f\" % (iter, num_batch_test, np.mean(loss_arr)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["이승아/checkpoint/U-Net/Dice/1e-4(origin)/model_epoch100.pth\n","Get saved weights successfully.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [00:56<00:00,  4.74s/it]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ki8Y2LrjooXV"},"source":["# Save final output\n"]},{"cell_type":"code","metadata":{"id":"Ftz-iFJ5qPUt"},"source":["# result_dir = './Training_Data/_output'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-fqnCL1pFDr"},"source":["# def save_final_output(name, output, origin_height, origin_width):\n","#     output *= 255\n","#     for idx in range(label.shape[0]):\n","#         file_name = name[idx]\n","\n","#         h = origin_height[idx]\n","#         w = origin_width[idx]\n","#         dim = (w, h)\n","\n","#         _output = output[idx].squeeze()\n","#         _output = cv.resize(_output, dim, interpolation = cv.INTER_NEAREST)\n","        \n","#         cv.imwrite(os.path.join(result_dir, file_name+'.png'), _output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_XEkTakpDhH"},"source":["# st_epoch=0\n","\n","# model, optim, st_epoch = load_model(ckpt_dir=ckpt_dir, model=model, optim=optim)\n","\n","# with torch.no_grad():\n","#     model.eval()\n","\n","#     loss_arr = []\n","#     total_recall = []\n","#     total_precision = []\n","#     total_f1_score = []\n","#     total_jaccard = []\n","\n","#     for iter, data in enumerate(tqdm(test_loader), 1):\n","#         name = data['name']\n","\n","#         origin_height = data['height']\n","#         origin_width = data['width']\n","\n","#         input = data['input'].to(device)\n","#         label = data['label'].to(device)\n","\n","#         output = model(input)\n","\n","#         loss = binary_focal_loss(output, label)\n","#         loss_arr += [loss.item()]\n","\n","#         # for metrics\n","#         # logit = torch.sigmoid(output)\n","#         # output = logit.clone()\n","#         output[output>0.5] = 1\n","#         output[output<=0.5] = 0\n","\n","#         # draw_ROC_curve(output, label)\n","#         # recall, precision, accuracy, f1_score, jaccard = get_score(output, label)\n","#         false_positive_rate,false_negtive_rate,f1_score = metric(label.cpu(), output.cpu())\n","        \n","#         label = fn_tonumpy(label)\n","#         input = fn_tonumpy(fn_denorm(input, mean=mean, std=std))\n","#         output = fn_tonumpy(output)\n","\n","#         save_final_output(name, output, origin_height, origin_width)\n","        \n","#         # save_predictedMask(input, label, output, iter)\n"],"execution_count":null,"outputs":[]}]}