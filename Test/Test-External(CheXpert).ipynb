{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Test-External(CheXpert).ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"14EaBaND8g20UjeXlyJyuduRxFW9KS9vA","authorship_tag":"ABX9TyO6ibsBZYaqI1gZB/9uU/UL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"NMXMkMMZ1OxK","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1636708310470,"user_tz":-540,"elapsed":19079,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"fdd83554-6dd7-4ae4-e5d9-957f74a69f38"},"source":["!pip install tqdm\n","!pip install segmentation_models_pytorch\n","!pip install albumentations\n","!pip install plantcv"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n","Requirement already satisfied: segmentation_models_pytorch in /usr/local/lib/python3.7/dist-packages (0.2.0)\n","Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.11.1+cu111)\n","Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.7.4)\n","Requirement already satisfied: efficientnet-pytorch==0.6.3 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.6.3)\n","Requirement already satisfied: timm==0.4.12 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.4.12)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (1.10.0+cu111)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.62.3)\n","Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (3.10.0.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.19.5)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n","Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n","Requirement already satisfied: imgaug<0.2.7,>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.2.6)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n","Collecting opencv-python\n","  Downloading opencv_python-4.5.4.58-cp37-cp37m-manylinux2014_x86_64.whl (60.3 MB)\n","\u001b[K     |████████████████████████████████| 60.3 MB 63 kB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.15.0)\n","Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.2.2)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.1.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.6.3)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.2.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.7)\n","Installing collected packages: opencv-python\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","plantcv 3.13.2 requires opencv-python<4,>=3.4, but you have opencv-python 4.5.4.58 which is incompatible.\u001b[0m\n","Successfully installed opencv-python-4.5.4.58\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["cv2"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: plantcv in /usr/local/lib/python3.7/dist-packages (3.13.2)\n","Collecting opencv-python<4,>=3.4\n","  Using cached opencv_python-3.4.16.57-cp37-cp37m-manylinux2014_x86_64.whl (58.0 MB)\n","Requirement already satisfied: dask in /usr/local/lib/python3.7/dist-packages (from plantcv) (2021.11.1)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from plantcv) (1.19.5)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from plantcv) (2.8.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from plantcv) (1.1.5)\n","Requirement already satisfied: plotnine in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.6.0)\n","Requirement already satisfied: dask-jobqueue in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.7.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from plantcv) (1.4.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.22.2.post1)\n","Requirement already satisfied: scikit-image>=0.13 in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.16.2)\n","Requirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (from plantcv) (0.10.2)\n","Requirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.7/dist-packages (from plantcv) (3.2.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->plantcv) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->plantcv) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5->plantcv) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->plantcv) (1.15.0)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13->plantcv) (2.4.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13->plantcv) (7.1.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13->plantcv) (2.6.3)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.13->plantcv) (1.2.0)\n","Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask->plantcv) (2.0.0)\n","Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask->plantcv) (0.11.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask->plantcv) (21.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from dask->plantcv) (3.13)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask->plantcv) (2021.11.0)\n","Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask->plantcv) (1.2.0)\n","Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask->plantcv) (0.2.1)\n","Requirement already satisfied: distributed>=2.19 in /usr/local/lib/python3.7/dist-packages (from dask-jobqueue->plantcv) (2021.11.1)\n","Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (2.4.0)\n","Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (5.4.8)\n","Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (1.0.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (57.4.0)\n","Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (2.0.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (1.7.0)\n","Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (5.1.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (2.11.3)\n","Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.19->dask-jobqueue->plantcv) (7.1.2)\n","Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.19->dask-jobqueue->plantcv) (1.0.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed>=2.19->dask-jobqueue->plantcv) (2.0.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->plantcv) (2018.9)\n","Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from plotnine->plantcv) (0.5.2)\n","Requirement already satisfied: mizani>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from plotnine->plantcv) (0.6.0)\n","Requirement already satisfied: descartes>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from plotnine->plantcv) (1.1.0)\n","Requirement already satisfied: palettable in /usr/local/lib/python3.7/dist-packages (from mizani>=0.6.0->plotnine->plantcv) (3.3.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->plantcv) (1.1.0)\n","Installing collected packages: opencv-python\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.5.4.58\n","    Uninstalling opencv-python-4.5.4.58:\n","      Successfully uninstalled opencv-python-4.5.4.58\n","Successfully installed opencv-python-3.4.16.57\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["cv2"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"Ze4jBsbgpKyg","executionInfo":{"status":"ok","timestamp":1636708312212,"user_tz":-540,"elapsed":1744,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["import os\n","import glob\n","import torch\n","import cv2 as cv\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","from scipy.misc import electrocardiogram\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from torch.utils.data import Dataset\n","from plantcv import plantcv as pcv\n","from skimage import img_as_float, morphology\n","from skimage.color import gray2rgb"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSRtacK0oMJi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636708312212,"user_tz":-540,"elapsed":10,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"3fef1d51-2da1-4059-b02b-a90cf3f317d0"},"source":["cd drive/Shareddrives/KOHI_의료영상1팀/"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Shareddrives/KOHI_의료영상1팀\n"]}]},{"cell_type":"code","metadata":{"id":"ylym7hfxoZff","executionInfo":{"status":"ok","timestamp":1636708312212,"user_tz":-540,"elapsed":8,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["evaluation = 'CheXpert'"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WqqMIPxHC_vy","executionInfo":{"status":"ok","timestamp":1636708312213,"user_tz":-540,"elapsed":9,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"ef4cd4b2-d446-45a0-f2c6-68412f9285cf"},"source":["len(os.listdir(f'External Validation/{evaluation}/dehazing'))"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["220"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"kxoJlFXqu1xM","executionInfo":{"status":"ok","timestamp":1636708312213,"user_tz":-540,"elapsed":8,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["image_dir = f'External Validation/{evaluation}/dehazing'"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YyPvVAG-0G4m"},"source":["## Load Image"]},{"cell_type":"markdown","metadata":{"id":"SSRuoUB-RnQp"},"source":["load image를 할때 cv.imread로 미리 읽어오면 메모리가 부족할 수도 있는 현상이 발생할 수도 있으므로,\n","\n","imread는 dataloader의 __getitem__에서 해결"]},{"cell_type":"code","metadata":{"id":"lzKZvo9KITwf","executionInfo":{"status":"ok","timestamp":1636708312213,"user_tz":-540,"elapsed":8,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["input_list = os.listdir(image_dir)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E4DkJBhPn_OO","executionInfo":{"status":"ok","timestamp":1636708312213,"user_tz":-540,"elapsed":7,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"64cb64ac-bd0a-4a86-9335-a43d6b73fe79"},"source":["len(input_list)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["220"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c6hkgjGWpalf","executionInfo":{"status":"ok","timestamp":1636708312214,"user_tz":-540,"elapsed":8,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"ab9b48e1-81c4-49a3-e6e3-0d4a6c575041"},"source":["dataset = []\n","\n","\n","for input in tqdm(input_list):\n","  image = os.path.join(image_dir, input)\n","\n","  dataset.append({'name': input[:-4], 'image_path':image})"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 220/220 [00:00<00:00, 131896.35it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"nb2jC_IF5Yb0"},"source":["### split train/test data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7z8fkLH35XvA","executionInfo":{"status":"ok","timestamp":1636708312214,"user_tz":-540,"elapsed":6,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"1dd84abd-9214-4161-f490-3e7e9c7891a4"},"source":["data_test = dataset\n","print(f'test dataset 개수: {len(data_test)}')\n","\n","lr = 1e-5\n","batch_size=8\n","train_continue= False\n","augment=False\n","\n","optim_mode = 'adam'\n","\n","model_sort = 'efficient'\n","\n","# ckpt_dir=f'이승아/checkpoint/U-Net/Dice/1e-05(padding)'\n","# result_dir =f'이승아/result/U-Net/Dice/1e-05(padding)'\n","\n","ckpt_dir=f'이승아/checkpoint/EfficientNet/V2'\n","result_dir =f'Evaluate result/{evaluation}/mask'\n","\n","image_size = 256\n"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["test dataset 개수: 220\n"]}]},{"cell_type":"markdown","metadata":{"id":"gFUf7rQjJIF5"},"source":["## Augmentation"]},{"cell_type":"code","metadata":{"id":"0D8ERhw48qcd","executionInfo":{"status":"ok","timestamp":1636708312214,"user_tz":-540,"elapsed":5,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["import albumentations as A\n","from torch.utils.data import DataLoader"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMYX2TfM9F_1","executionInfo":{"status":"ok","timestamp":1636708312214,"user_tz":-540,"elapsed":5,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def _normalization(input):\n","  input = (input - input.min()) / (input.max() - input.min())\n","  return input\n","\n","def _standardization(input):\n","  input = (input - mean) / std\n","  return input\n","\n","def _to_tensor(input, name, h, w):\n","  input = input.astype('float32')\n","  input = input.reshape((1, input.shape[0], input.shape[1]))\n","\n","  data = {'name':name, 'input': torch.from_numpy(input), 'height': h, 'width': w}\n","  return data\n","\n","def _random_augment(input):\n","  h, w  = input.shape\n","  \n","  transform = A.Compose([A.HorizontalFlip(p=0.5),\n","                      A.VerticalFlip(p=0.5),\n","                      A.RandomCrop(height=int(h*0.8), width=int(w*0.8), p=0.5)\n","                      ])\n","\n","  augmented = transform(image=input)\n","\n","  input = augmented['image']\n","\n","  return input"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ccAqrF70tcST"},"source":["## Resize by aspect ratio and Padding Image"]},{"cell_type":"code","metadata":{"id":"jwq3WIQStbEa","executionInfo":{"status":"ok","timestamp":1636708312214,"user_tz":-540,"elapsed":5,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def resize_image(input):\n","  # shape: (height, width, channel)\n","  # aspect ratio를 고려하여 resize\n","  if input.shape[1] > input.shape[0]:   # height를 기준으로\n","    r = image_size / input.shape[1]\n","    dim = ( image_size, int(input.shape[0] * r))\n","  else:                                 # width를 기준으로\n","    r = image_size / input.shape[0]\n","    dim = (int(input.shape[1] * r), image_size)\n","    \n","  resized_input = cv.resize(input, dim, interpolation = cv.INTER_AREA)\n","  \n","  return resized_input\n","\n","def padding_image(input):\n","  input_size = input.shape\n","  target_size = (image_size, image_size)\n","\n","  if input_size[1] < image_size:\n","    padding_range = int(target_size[1]-input_size[1])\n","  elif input_size[0] < image_size:\n","    padding_range = int(target_size[0]-input_size[0])\n","  else:\n","    return input\n","\n","  if padding_range%2 == 0:\n","    padding_size = (int(padding_range/2), int(padding_range/2))\n","  else:\n","    padding_size = (int(padding_range/2), int(padding_range/2)+1)\n","  \n","  if input_size[1] < image_size:\n","    npad= ((0,0),padding_size)\n","  elif input_size[0] < image_size:\n","    npad= (padding_size, (0,0))\n","\n","  padding_input = np.pad(input, npad,'constant', constant_values=(0))\n","\n","  return padding_input"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UM4PYZNw47T7"},"source":["## DataLoader"]},{"cell_type":"code","metadata":{"id":"U05CJ7uh27bq","executionInfo":{"status":"ok","timestamp":1636708312215,"user_tz":-540,"elapsed":6,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["# DataLoader\n","\n","class VertebraeDataset(Dataset):\n","\n","  def __init__(self, data, augment=False):\n","    super(VertebraeDataset, self).__init__()\n","\n","    self.data = data      \n","    self.augment = augment\n","    self.image_size = image_size\n","\n","  def __getitem__(self, index):\n","    # Read input, label\n","    name = self.data[index]['name']\n","    input = cv.imread(self.data[index]['image_path'])\n","    h, w, _ = input.shape\n","    try:\n","      input = cv.cvtColor(input, cv.COLOR_BGR2GRAY)    \n","    except:\n","      print(self.data[index]['image_path'])\n","      exit(-1)\n","\n","    # # 영상 개선\n","    clahe = cv.createCLAHE(clipLimit=2.0, tileGridSize=(16, 16))\n","    input = clahe.apply(input)\n","    input = input / 255.0\n","    \n","    if self.augment:\n","      input = _random_augment(input)\n","      \n","    # Resize and pad Image\n","    input = resize_image(input)\n","    input = padding_image(input)\n","\n","    data = _to_tensor(input, name, h, w)\n","\n","    return data\n","\n","  def __len__(self):\n","    return len(self.data)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sja5vP-y6Hs2","executionInfo":{"status":"ok","timestamp":1636708312845,"user_tz":-540,"elapsed":636,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def get_data(data, shuffle=True):\n","    ### Train Dataset 가져오기\n","    dataset = VertebraeDataset(data, augment=augment)\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)  \n","\n","    return dataset, loader\n","\n","# pytorch Dataloader\n","test_dataset, test_loader = get_data(data_test, shuffle=False)\n","\n","_data = test_dataset.__getitem__(0)\n","\n","\n","# test_dataset, test_loader = get_data(data_train[:1])"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8QfXtubLHJT"},"source":["## Parameter 설정\n"]},{"cell_type":"code","metadata":{"id":"JqQe3tOH2Kqw","executionInfo":{"status":"ok","timestamp":1636708313844,"user_tz":-540,"elapsed":1000,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["import torch.nn.functional as F\n","import segmentation_models_pytorch as smp\n","from torch import nn"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"00eUF-si1h2J","executionInfo":{"status":"ok","timestamp":1636708313844,"user_tz":-540,"elapsed":3,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","class DiceLoss(nn.Module):\n","    \"\"\"\n","    Dice score loss function\n","    \"\"\"\n","    def __init__(self):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = 1.0\n","\n","    def forward(self, output, label):\n","        assert output.size() == label.size()\n","        output = output[:, 0].contiguous().view(-1)\n","        label = label[:, 0].contiguous().view(-1)\n","        intersection = (output * label).sum()\n","        dsc = (2. * intersection + self.smooth) / (output.sum() + label.sum() + self.smooth)\n","\n","        return 1. - dsc\n","\n","\n","class DiceBCELoss(nn.Module):\n","    def __init__(self, weight=None, size_average=True):\n","        super(DiceBCELoss, self).__init__()\n","\n","    def forward(self, inputs, targets, smooth=1):\n","\n","        # comment out if your model contains a sigmoid or equivalent activation layer\n","        inputs = torch.sigmoid(inputs)\n","\n","        # flatten label and prediction tensors\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice_loss = 1 - (2. * intersection + smooth) /(inputs.sum() + targets.sum() + smooth)\n","        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n","        Dice_BCE = BCE + dice_loss\n","\n","        return Dice_BCE\n","\n","class Binary_Loss(nn.Module):\n","    def __init__(self):\n","        super(Binary_Loss, self).__init__()\n","        self.criterion = nn.BCEWithLogitsLoss()\n","\n","\n","    def forward(self, model_output, targets):\n","        loss = self.criterion(model_output, targets)\n","\n","       \n","        return loss\n","\n","bcedice_loss = DiceBCELoss().cuda()\n","binary_loss = Binary_Loss().cuda()\n","dsc_loss = DiceLoss().cuda()"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9zjD6UidJJpi"},"source":["# Model"]},{"cell_type":"code","metadata":{"id":"t7Ml0exeJLqf","executionInfo":{"status":"ok","timestamp":1636708313844,"user_tz":-540,"elapsed":2,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["from segmentation_models_pytorch.unet.model import Unet"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"1LCPAOMgY8ac","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636708317370,"user_tz":-540,"elapsed":3528,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"504c7d54-2114-4a31-eb2f-fb07cebea927"},"source":["from collections import OrderedDict\n","\n","if model_sort == 'unet':\n","  class UNet(nn.Module):\n","\n","      def __init__(self, in_channels=1, out_channels=1, init_features=32):\n","          super(UNet, self).__init__()\n","\n","          features = init_features\n","          self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n","          self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","          self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n","          self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n","          self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n","          self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n","          self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n","          self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","          self.bottleneck = UNet._block(features * 8, features * 16, name=\"bottleneck\")\n","\n","          self.upconv4 = nn.ConvTranspose2d(\n","              features * 16, features * 8, kernel_size=2, stride=2\n","          )\n","          self.decoder4 = UNet._block((features * 8) * 2, features * 8, name=\"dec4\")\n","          self.upconv3 = nn.ConvTranspose2d(\n","              features * 8, features * 4, kernel_size=2, stride=2\n","          )\n","          self.decoder3 = UNet._block((features * 4) * 2, features * 4, name=\"dec3\")\n","          self.upconv2 = nn.ConvTranspose2d(\n","              features * 4, features * 2, kernel_size=2, stride=2\n","          )\n","          self.decoder2 = UNet._block((features * 2) * 2, features * 2, name=\"dec2\")\n","          self.upconv1 = nn.ConvTranspose2d(\n","              features * 2, features, kernel_size=2, stride=2\n","          )\n","          self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n","\n","          self.conv = nn.Conv2d(\n","              in_channels=features, out_channels=out_channels, kernel_size=1\n","          )\n","\n","      def forward(self, x):\n","          enc1 = self.encoder1(x)\n","          enc2 = self.encoder2(self.pool1(enc1))\n","          enc3 = self.encoder3(self.pool2(enc2))\n","          enc4 = self.encoder4(self.pool3(enc3))\n","\n","          bottleneck = self.bottleneck(self.pool4(enc4))\n","\n","          dec4 = self.upconv4(bottleneck)\n","          dec4 = torch.cat((dec4, enc4), dim=1)\n","          dec4 = self.decoder4(dec4)\n","          dec3 = self.upconv3(dec4)\n","          dec3 = torch.cat((dec3, enc3), dim=1)\n","          dec3 = self.decoder3(dec3)\n","          dec2 = self.upconv2(dec3)\n","          dec2 = torch.cat((dec2, enc2), dim=1)\n","          dec2 = self.decoder2(dec2)\n","          dec1 = self.upconv1(dec2)\n","          dec1 = torch.cat((dec1, enc1), dim=1)\n","          dec1 = self.decoder1(dec1)\n","          return torch.sigmoid(self.conv(dec1))\n","\n","      @staticmethod\n","      def _block(in_channels, features, name):\n","          return nn.Sequential(\n","              OrderedDict(\n","                  [\n","                      (\n","                          name + \"conv1\",\n","                          nn.Conv2d(\n","                              in_channels=in_channels,\n","                              out_channels=features,\n","                              kernel_size=3,\n","                              padding=1,\n","                              bias=False,\n","                          ),\n","                      ),\n","                      (name + \"norm1\", nn.BatchNorm2d(num_features=features)),\n","                      (name + \"relu1\", nn.ReLU(inplace=True)),\n","                      (\n","                          name + \"conv2\",\n","                          nn.Conv2d(\n","                              in_channels=features,\n","                              out_channels=features,\n","                              kernel_size=3,\n","                              padding=1,\n","                              bias=False,\n","                          ),\n","                      ),\n","                      (name + \"norm2\", nn.BatchNorm2d(num_features=features)),\n","                      (name + \"relu2\", nn.ReLU(inplace=True)),\n","                  ]\n","              )\n","          )\n","  model = UNet().to(device)\n","  print('unet')\n","else:\n","  model = Unet(\n","    encoder_name=\"efficientnet-b5\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n","    encoder_weights=None,     # use `imagenet` pre-trained weights for encoder initialization\n","    in_channels=1,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n","    classes=1,).to(device)\n","  print('efficient')\n","\n","# Optimizer Adam 설정\n","if optim_mode == 'adam':\n","  optim = torch.optim.Adam(model.parameters(), lr=lr)\n","elif optim_mode == 'sgd':\n","  optim = torch.optim.SGD(model.parameters(), lr=lr)\n","elif optim_mode == 'adamW':\n","  optim = torch.optim.Adam(model.parameters(), lr=lr)\n","print(optim_mode)"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["efficient\n","adam\n"]}]},{"cell_type":"code","metadata":{"id":"O3A2ecqdPuUt","executionInfo":{"status":"ok","timestamp":1636708317371,"user_tz":-540,"elapsed":11,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)  # Tensor를 numpy로 변환\n","fn_denorm = lambda x, mean, std: (x * std) + mean  # DeNomarlization\n","fn_class = lambda x: 1.0 * (x > 0.4)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"_6Jy2ctTXEv_","executionInfo":{"status":"ok","timestamp":1636708317371,"user_tz":-540,"elapsed":10,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def load_model(ckpt_dir, model, optim):\n","    if not os.path.exists(ckpt_dir):\n","        epoch = 0\n","        return model\n","\n","    ckpt_lst = os.listdir(ckpt_dir)\n","    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n","    print(os.path.join(ckpt_dir, ckpt_lst[-1]))\n","\n","    dict_model = torch.load(os.path.join(ckpt_dir, ckpt_lst[-1]))\n","    model.load_state_dict(dict_model['model'], strict=False)\n","    optim.load_state_dict(dict_model['optim'])\n","    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n","    print(\"Get saved weights successfully.\")\n","\n","    return model, optim, epoch\n","\n","def save_model(ckpt_dir, model, optim, epoch):\n","    if not os.path.exists(ckpt_dir):\n","        os.makedirs(ckpt_dir)\n","\n","    torch.save({'model': model.state_dict(), 'optim': optim.state_dict()},\n","                \"./%s/model_epoch%d.pth\" % (ckpt_dir, epoch))\n","    print(f'>> save model_{epoch}.pth')"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GxAdDAPKYbO0"},"source":["# Test"]},{"cell_type":"code","metadata":{"id":"h_i-iyqjXUa7","executionInfo":{"status":"ok","timestamp":1636708317371,"user_tz":-540,"elapsed":9,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def numeric_score(output, label):\n","    FP = np.float(np.sum((output == 1) & (label == 0)))\n","    FN = np.float(np.sum((output == 0) & (label == 1)))\n","    if FP != 0.0 or FN != 0.0:\n","        pass\n","    TP = np.float(np.sum((output == 1) & (label == 1)))\n","    TN = np.float(np.sum((output == 0) & (label == 0)))\n","\n","    return FP, FN, TP, TN\n","\n","def get_score(output, label):\n","    FP, FN, TP, TN = numeric_score(output, label)\n","    N = FP + FN + TP + TN\n","\n","    epsilon = 1e-5\n","\n","    # Recall : TP / TP+FN\n","    recall = np.divide(TP, TP + FN + epsilon)\n","    # Precision : TP / TP+FP\n","    precision = np.divide(TP, (TP+FP+epsilon))\n","\n","    accuracy = np.divide((TP + TN), N+epsilon)\n","\n","    # F1 socre = 2 * (A interect B) / |A| + |B| = 2TP / 2TP + FP + FN\n","    f1_score = 2 * (precision*recall) / (precision + recall + epsilon)\n","    dice_coeff = 2*TP / (2*TP+FP+FN+epsilon)\n","\n","    # J(A,B) = | A intersect B | / | A union B |\n","    jaccard_score = TP / (TP+FN+FP+ epsilon)\n","\n","    return recall * 100, precision * 100, accuracy * 100, f1_score*100, jaccard_score*100\n"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"IkFoH0nJdnc9","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1636708317371,"user_tz":-540,"elapsed":9,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"c9769da1-6e42-4d7f-ec18-334852e577b5"},"source":["result_dir"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Evaluate result/CheXpert/mask'"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"MPXck0Vmo_sJ","executionInfo":{"status":"ok","timestamp":1636708317373,"user_tz":-540,"elapsed":10,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["import copy\n","\n","def metric(gt,pred):\n","    preds = pred.detach().numpy()\n","    gts = gt.detach().numpy()\n","\n","    pred = preds.astype(int)  # float data does not support bit_and and bit_or\n","    gdth = gts.astype(int)  # float data does not support bit_and and bit_or\n","    fp_array = copy.deepcopy(pred)  # keep pred unchanged\n","    fn_array = copy.deepcopy(gdth)\n","    gdth_sum = np.sum(gdth)\n","    pred_sum = np.sum(pred)\n","    intersection = gdth & pred\n","    union = gdth | pred\n","    intersection_sum = np.count_nonzero(intersection)\n","    union_sum = np.count_nonzero(union)\n","\n","    tp_array = intersection\n","\n","    tmp = pred - gdth\n","    fp_array[tmp < 1] = 0\n","\n","    tmp2 = gdth - pred\n","    fn_array[tmp2 < 1] = 0\n","\n","    tn_array = np.ones(gdth.shape) - union\n","\n","    tp, fp, fn, tn = np.sum(tp_array), np.sum(fp_array), np.sum(fn_array), np.sum(tn_array)\n","\n","    smooth = 0.001\n","    precision = tp / (pred_sum + smooth)\n","    recall = tp / (gdth_sum + smooth)\n","\n","    false_positive_rate = fp / (fp + tn + smooth)\n","    false_negtive_rate = fn / (fn + tp + smooth)\n","\n","    jaccard = intersection_sum / (union_sum + smooth)\n","    dice = 2 * intersection_sum / (gdth_sum + pred_sum + smooth)\n","    \n","    epsilon = 1e-5\n","    f1_score = 2 * (precision*recall) / (precision + recall + epsilon)\n","\n","    return false_positive_rate,false_negtive_rate,f1_score\n","    "],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"8JisjWDX-y-H","executionInfo":{"status":"ok","timestamp":1636708342279,"user_tz":-540,"elapsed":460,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def _normalize_image(image, *, image_cmap=None):\n","    image = img_as_float(image)\n","    if image.ndim == 2:\n","        if image_cmap is None:\n","            image = gray2rgb(image)\n","        else:\n","            image = plt.get_cmap(image_cmap)(image)[..., :3]\n","    return image"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"evx0hx2D-z3S","executionInfo":{"status":"ok","timestamp":1636708344807,"user_tz":-540,"elapsed":453,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["def overlay_skeleton_2d(image, skeleton, *,\n","                        image_cmap=None, color=(1, 0, 0), alpha=1,\n","                        dilate=0, axes=None):\n","    image = _normalize_image(image, image_cmap=image_cmap)\n","    skeleton = skeleton.astype(bool)\n","    if dilate > 0:\n","        selem = morphology.disk(dilate)\n","        skeleton = morphology.binary_dilation(skeleton, selem)\n","    image[skeleton] = alpha * np.array(color) + (1 - alpha) * image[skeleton]\n","    return image"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"a_FWnwvtaQV3","executionInfo":{"status":"ok","timestamp":1636708317373,"user_tz":-540,"elapsed":9,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["# !pip uninstall -y opencv-python opencv-contrib-python"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"UNnveONEacdw","executionInfo":{"status":"ok","timestamp":1636708317374,"user_tz":-540,"elapsed":10,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["# !pip install opencv-contrib-python"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"4U0aedJDPc0P","executionInfo":{"status":"ok","timestamp":1636708317374,"user_tz":-540,"elapsed":10,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["from cv2.ximgproc import thinning\n","\n","def remove_horizontal_line(output):\n","  line_kernel = np.zeros((7, 7),dtype=np.uint8)\n","  line_kernel[3,...]=1\n","  x=cv.morphologyEx(output, cv.MORPH_OPEN, line_kernel ,iterations=1)\n","  _output= output - x\n","\n","  return _output "],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"iqU9hnYypf7E","executionInfo":{"status":"ok","timestamp":1636708428855,"user_tz":-540,"elapsed":624,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["final_result_dir = './Evaluate result/PadChest/line'\n","\n","def post_processing(output):\n","  \n","  # # Opening Image\n","  kernel = np.ones((9, 9), np.uint8)\n","  result_opening = cv.morphologyEx(output, cv.MORPH_OPEN, kernel)\n","  \n","  # Thining Image\n","  result_thinning = thinning((result_opening*255).astype('uint8'))\n","  # Remove horizontal_line\n","  result_thinning = remove_horizontal_line(result_thinning)\n","\n","  # # Skeletonizing Image\n","  # result_skeletonizing = (skeletonize(result_opening)*255).astype('uint8')\n","\n","  return result_opening, result_thinning\n","\n","def save_predictedMask(input, output, name, origin_height, origin_width):\n","    for idx in range(input.shape[0]):\n","        file_name = name[idx]\n","        _input = input[idx].astype('uint8').squeeze()\n","\n","        result_opening, result_thinning = post_processing(output[idx].squeeze())\n","        # pruned_skeleton, segmented_img, segment_objects = pcv.morphology.prune(skel_img=result_skeletonizing, size=300)\n","\n","        overlay_image = overlay_skeleton_2d(_input, result_thinning, dilate=1)\n","        \n","        cv.imwrite(os.path.join('./Evaluate result/CheXpert/mask', file_name+'.png'), result_opening*255)\n","        cv.imwrite(os.path.join('./Evaluate result/CheXpert/line/256', file_name+'.png'), result_thinning)\n","        \n","        h = origin_height[idx].item()\n","        w = origin_width[idx].item()\n","        dim = (w, h)\n","        resized_result_thinning = cv.resize(result_thinning, dim, interpolation = cv.INTER_NEAREST)\n","        cv.imwrite(os.path.join('./Evaluate result/CheXpert/line/origin_size', file_name+'.png'), resized_result_thinning)\n","        \n","        fig, ax = plt.subplots()\n","        plt.axis('off'), plt.xticks([]), plt.yticks([])\n","        plt.tight_layout()\n","        plt.imshow(overlay_image)\n","        plt.savefig(os.path.join('./Evaluate result/CheXpert/overlay', file_name+'.png'),bbox_inches='tight')\n","        plt.close()"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"zBQ_Qrw98HGS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636709635619,"user_tz":-540,"elapsed":1205149,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}},"outputId":"59fedd10-0808-4fef-a5f0-50cd585a007b"},"source":["st_epoch=0\n","\n","print(result_dir)\n","model, optim, st_epoch = load_model(ckpt_dir=ckpt_dir, model=model, optim=optim)\n","\n","with torch.no_grad():\n","    model.eval()\n","\n","    loss_arr = []\n","    total_recall = []\n","    total_precision = []\n","    total_f1_score = []\n","    total_jaccard = []\n","\n","    for iter, data in enumerate(tqdm(test_loader), 1):\n","        name = data['name']\n","        input = data['input'].to(device)\n","\n","        origin_height = data['height']\n","        origin_width = data['width']\n","        \n","        output = model(input)\n","\n","        # for metrics\n","        logit = torch.sigmoid(output)\n","        output = logit.clone()\n","        output[output>0.5] = 1\n","        output[output<=0.5] = 0\n","\n","        input = fn_tonumpy(input*255)\n","        output = fn_tonumpy(output)\n","        \n","        save_predictedMask(input, output, name, origin_height, origin_width)"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluate result/CheXpert/mask\n","이승아/checkpoint/EfficientNet/V2/model_epoch300.pth\n","Get saved weights successfully.\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28/28 [20:04<00:00, 43.00s/it]\n"]}]},{"cell_type":"code","metadata":{"id":"bPHxGiQGSDwJ","executionInfo":{"status":"aborted","timestamp":1636706172867,"user_tz":-540,"elapsed":7,"user":{"displayName":"승아","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLe0GoZZe5YdrnY6MJpIFgIzfBdXfUJ_X0tiWp=s64","userId":"08698022982440908757"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]}]}